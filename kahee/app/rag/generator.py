## LLM ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
## ì´í›„ ìˆ˜ì • í•„ìš”ìš”

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨ìŒ
from typing import List
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain.schema import Document


# í”„ë¡¬í”„íŠ¸ ìž‘ì„±
_llm = ChatOpenAI(model_name="gpt-4o", temperature=0.2)

PROMPT = """ë‹¹ì‹ ì€ ê´€ì ˆ, ë¼ˆ, ê·¼ìœ¡, ë‡Œ, ì†Œí™”ê³„ ë“±ì˜ ì¦ìƒì— ë”°ë¼ ì ì ˆí•œ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆì„ ì¶”ì²œí•˜ëŠ” ì˜ì–‘ì œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ, ì•„ëž˜ í˜•ì‹ì— ë”°ë¼ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸:
{question}

ðŸ“Œ ë¬¸ì œ ì„¤ëª… (í•´ë‹¹ ì¦ìƒê³¼ ê´€ë ¨ëœ ì›ì¸, ê¸°ì „ ë“± ìš”ì•½)
## ì¶”ì²œ ê¸°ëŠ¥ì„± ì„±ë¶„
| ì„±ë¶„ëª… | ê¸°ëŠ¥ì„± ì„¤ëª… |
|--------|--------------|
| ì˜ˆì‹œì„±ë¶„ | í•´ë‹¹ ì„±ë¶„ì´ ì™œ ë„ì›€ì´ ë˜ëŠ”ì§€ ì„¤ëª… |
...

## ì¶”ì²œ ì˜ì–‘ì œ ë¦¬ìŠ¤íŠ¸
### 1. ì‹¤ì œ ì œí’ˆëª…
      (ì˜ˆ: "1. ë§¤ìŠ¤í‹± ê²€ ì œí’ˆ âŒ" â†’ "1. ë§¤ìŠ¤í‹±ê°€ë“œí”ŒëŸ¬ìŠ¤ âœ…")
- âœ… ì„±ë¶„:
- ðŸ’Š ë³µìš©ë²•:
- âš ï¸ ì£¼ì˜:

...

## ðŸ“Œ ë³µì•½ ì•ˆë‚´
- íš¨ê³¼ê°€ ë‚˜íƒ€ë‚˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„
- ë³µìš© íŒ (ì‹ì „/í›„, ìˆ˜ë¶„ ì„­ì·¨ ë“±)
- ë¶€ìž‘ìš© ì£¼ì˜ì‚¬í•­

ðŸ“š ì°¸ê³ ë¬¸í—Œ: ì œí’ˆ DB, ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì •ë³´ í¬í„¸ ë“±
"""

def generate_answer(context_docs: List[Document], question: str) -> str:
    context = "\n".join([d.page_content for d in context_docs])
    prompt = PROMPT.format(context=context, question=question)
    return _llm.invoke(prompt).content.strip()

